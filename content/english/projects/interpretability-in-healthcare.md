---

title: "Interpretability of Deep Learning Models in Healthcare"
# image: "https://images.unsplash.com/photo-1560790189-429f6daad014?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1170&q=80"
image: "https://rbcdsai.iitm.ac.in/images/research/Interpretability-of-Deep-Learning-Models-in-Healthcare.jpg"
draft: false
---

Interpretability of deep learning models is essential for widespread adoption of these techniques in the Medical image diagnosis community. Deep learning models have been phenomenally successful at beating state of the art in common medical image diagnosis tasks like segmentation as well as in screening applications, for e.g. classification of diabetic retinopathy, diagnosis of chest X-ray scans among others. While these successes have created huge interest in adopting these techniques in clinical practice, a huge barrier in adoption is the lack of interpretability of these models. Convolutional Neural Networks with hundreds of layers is the workhorse for medical image diagnosis. While the initial layers are typically edge detectors and shape detectors, as one goes deeper into the network it is fairly impossible to explain or interpret the feature maps. In order for clinicians to trust the output from these networks it is essential that a mechanism for explaining the output be present. Black box techniques will make it hard for clinicians to justify the diagnosis as well as for follow up procedures.

In this proposal we seek to extend activation maximation to problems in the medical image analysis domain by incorporating domain data specific constraints in the loss function for generating synthetic inputs. The feasibility of this approach stems from the fact that even though there are variations in human anatomy between individuals, overall the degree of variation is quite low and in most cases the shape and appearance of the anatomy in the image does not vary much between patients. One can exploit this low variability to constrain the synthesized image to match a shape template obtained from the training data set. Statistical shape and appearance models are often used in medical image analysis for segmentation tasks and methodology for estimating these models have been well studied.

We propose to incorporate shape and appearance models proposed recently. as a regularization term in the activation maximization model so as to constrain the synthesized images to be clinically meaningful. The regularization can be affected by directly using the L2 norm of the difference between the synthesized image and the model. Alternatively, an auto-encoder based on the training data can be used to extract low dimensional representation of the images and a statistical measure like KL divergence between the model and the synthesized image can act as a regularizer.

Initial studies on brain tumor segmentation using the na√Øve activation maximization have yielded promising results with early layers capturing the overall shape of the brain while successive layers focus on the tumor. However, the results are not consistent and, in some cases, not interpretable. We hypothesize that Incorporating shape and appearance priors in the loss function will lead to the synthesis of images that are physically meaningful and improve the interpretability of the network prediction. We will incorporate shape and appearance priors for both brain tumor segmentation and cardiac segmentation tasks. The work is expected to establish a technique for interpreting the outputs of deep neural networks which are beating state of the art for many routine medical image diagnosis tasks.